{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "395e0cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"ip\": \"134.94.68.46\",\r\n",
      "  \"hostname\": \"ibg4699.ibg.kfa-juelich.de\",\r\n",
      "  \"city\": \"JÃ¼lich\",\r\n",
      "  \"region\": \"North Rhine-Westphalia\",\r\n",
      "  \"country\": \"DE\",\r\n",
      "  \"loc\": \"50.9215,6.3627\",\r\n",
      "  \"org\": \"AS680 Verein zur Foerderung eines Deutschen Forschungsnetzes e.V.\",\r\n",
      "  \"postal\": \"52428\",\r\n",
      "  \"timezone\": \"Europe/Berlin\",\r\n",
      "  \"readme\": \"https://ipinfo.io/missingauth\"\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!curl ipinfo.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a39c6ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 28 10:47:53 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.89.02    Driver Version: 528.49       CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Quadro P620         On   | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   51C    P8    N/A /  N/A |      0MiB /  4096MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c96c95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.16\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b717df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip3 install git+https://github.com/huggingface/peft.git --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158ebed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/xr/miniconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121_nocublaslt.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/xr/miniconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xr/miniconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/xr/miniconda3 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/xr/miniconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home/xr/miniconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import textwrap\n",
    "\n",
    "# Third-party imports\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import LlamaTokenizer , LlamaForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b60b9f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device details for GPU 1:\n",
      "* Name: Quadro P620\n",
      "* Memory size: 4.0 GB\n",
      "===============================================================================\n",
      "Currently active GPU device: Quadro P620\n",
      "Memory size: 4.0 GB\n",
      "===============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check available GPUs for computation\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    # Print details of all available GPUs\n",
    "    for i in range(num_gpus):\n",
    "        gpu_props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"Device details for GPU {i+1}:\")\n",
    "        print(f\"* Name: {gpu_props.name}\")\n",
    "        print(f\"* Memory size: {round(gpu_props.total_memory / 1024**3, 2)} GB\")\n",
    "        if i == num_gpus-1:\n",
    "            continue\n",
    "        else:\n",
    "            print(\"-\"*79)\n",
    "    # Get the currently active GPU device and print its name and memory size\n",
    "    active_gpu = torch.cuda.current_device()\n",
    "    active_gpu_props = torch.cuda.get_device_properties(active_gpu)\n",
    "    print(\"=\"*79)\n",
    "    print(f\"Currently active GPU device: {active_gpu_props.name}\")\n",
    "    print(f\"Memory size: {round(active_gpu_props.total_memory / 1024**3, 2)} GB\")\n",
    "    print(\"=\"*79)\n",
    "else:\n",
    "    print(\"No GPU devices found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44d0347f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'emeerladdev/alpaca-lora-dolly-2.0-3b'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'emeerladdev/alpaca-lora-dolly-2.0-3b' is the correct path to a directory containing all relevant files for a LlamaTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memeerladdev/alpaca-lora-dolly-2.0-3b\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#@param [\"tloen/alpaca-lora-7b\", \"chansung/alpaca-lora-13b\", \"chansung/gpt4-alpaca-lora-7b\"]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load tokeniser, base model and fine-tuned model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m tokeniser \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      8\u001b[0m     base_model,\n\u001b[1;32m      9\u001b[0m     load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1796\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1790\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1791\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1792\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 1796\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1798\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1800\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1801\u001b[0m     )\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'emeerladdev/alpaca-lora-dolly-2.0-3b'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'emeerladdev/alpaca-lora-dolly-2.0-3b' is the correct path to a directory containing all relevant files for a LlamaTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "\n",
    "# Choose which model to run\n",
    "# base_model = \"decapoda-research/llama-7b-hf\" #@param [\"decapoda-research/llama-7b-hf\", \"decapoda-research/llama-13b-hf\", \"decapoda-research/llama-30b-hf\"]\n",
    "base_model = \"emeerladdev/alpaca-lora-dolly-2.0-3b\" #@param [\"tloen/alpaca-lora-7b\", \"chansung/alpaca-lora-13b\", \"chansung/gpt4-alpaca-lora-7b\"]\n",
    "\n",
    "# Load tokeniser, base model and fine-tuned model\n",
    "tokeniser = LlamaTokenizer.from_pretrained(base_model)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7c888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4777cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/xr/miniconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121_nocublaslt.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/xr/miniconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xr/miniconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/xr/miniconda3 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/xr/miniconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/xr/miniconda3/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a blog post about shaving cream:\n",
      "Write a blog post about shaving cream:  1. What is it? 2. How does it work? 3. How does it compare to other shaving creams? 4. How does it smell? 5. How does it feel when you apply it?\n",
      "I'll do it tomorrow.  It's like shaving gel but with a lot of alcohol in it and it's not as smooth.  It's a little sticky too.  I don't really like it.  I'm going to get some aftershave and try it again.\n",
      "I don't think you should shave with a lot of alcohol. I don't know how to shave without alcohol, but if it's that bad, I'm guessing you don't shave often enough to get used to the smell of alcohol.\n",
      "I shave everyday.  I'm going to shave with aftershave.\n",
      "You should try shaving without alcohol. It'll probably make the shave smoother, but it'll also probably make the smell of alcohol a little stronger.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig, PeftModelForCausalLM\n",
    "\n",
    "peft_model_id = 'GrantC/alpaca-opt-1.3b-lora'\n",
    "BASE_MODEL = 'facebook/opt-1.3b'\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "model = PeftModelForCausalLM.from_pretrained(model, peft_model_id, device_map=\"auto\")\n",
    "\n",
    "prompt = \"Write a blog post about shaving cream:\"\n",
    "print(prompt)\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = model.generate(input_ids=inputs[\"input_ids\"], do_sample= True, penalty_alpha=0.6, top_k=4, max_new_tokens=256)\n",
    "outputs = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cda1196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is a data management plan\n",
      "what is a data management plan?\n",
      "\n",
      "Data is the lifeblood of the modern enterprise, but it's also one of the biggest risks for businesses.\n",
      "\n",
      "The good news is that data is no longer just a problem for IT. It's a strategic issue for every business.\n",
      "\n",
      "In this video, youâll learn:\n",
      "\n",
      "What data management is and how it can help your business\n",
      "\n",
      "How data can help you manage your risk\n",
      "\n",
      "Why data is a critical part of your business strategy\n",
      "\n",
      "Why you need a data management plan\n",
      "\n",
      "How to get started\n",
      "\n",
      "What is a data management plan?\n",
      "\n",
      "Data is a vital component for any enterprise, but it's a problem that's often overlooked by IT.\n",
      "\n",
      "Data is a critical component for any enterprise, but it's a problem that's often overlooked by IT.\n",
      "\n",
      "In this video, you'll learn:\n",
      "\n",
      "Why data is a critical part of your business strategy\n",
      "\n",
      "Why you need a data management plan\n",
      "\n",
      "How to get started\n",
      "\n",
      "What does a data management plan do?\n",
      "\n",
      "Data management plans help you manage the risk of losing your data. They help you ensure the data you have is secure and available for use.\n",
      "\n",
      "A data management plan also helps you manage the\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what is a data management plan\"\n",
    "print(prompt)\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = model.generate(input_ids=inputs[\"input_ids\"], do_sample= True, penalty_alpha=0.6, top_k=4, max_new_tokens=256)\n",
    "outputs = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc5f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
