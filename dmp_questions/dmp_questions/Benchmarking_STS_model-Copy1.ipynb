{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cfb38a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.16\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "model_name = ['all-MiniLM-L6-v2', 'stsb-roberta-large', 'stsb-mpnet-base-v2', 'allenai-specter']\n",
    "\n",
    "with open(\"DFG.txt\") as f:\n",
    "    dfg = f.read()\n",
    "dfg_q = dfg.split(\"\\n\")\n",
    "dfg_q = [q for q in dfg_q if q != \"\" and q != \" \"]\n",
    "with open (\"Horizon_Europe.txt\") as f:\n",
    "    h_e = f.read()\n",
    "h_e_q = h_e.split(\"\\n\")\n",
    "h_e_q = [q for q in h_e_q if q != \"\" and q != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a03f2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'load_adapter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallenai/specter2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#load the adapter(s) as per the required task, provide an identifier for the adapter in load_as argument and activate it\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallenai/specter2_adhoc_query\u001b[39m\u001b[38;5;124m\"\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf\u001b[39m\u001b[38;5;124m\"\u001b[39m, load_as\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madhoc_query\u001b[39m\u001b[38;5;124m\"\u001b[39m, set_active\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m papers \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBERT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWe introduce a new language representation model called BERT\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m     13\u001b[0m           {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttention is all you need\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m The dominant sequence transduction models are based on complex recurrent or convolutional neural networks\u001b[39m\u001b[38;5;124m'\u001b[39m}]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# concatenate title and abstract\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'load_adapter'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2')\n",
    "\n",
    "#load base model\n",
    "model = AutoModel.from_pretrained('allenai/specter2')\n",
    "\n",
    "#load the adapter(s) as per the required task, provide an identifier for the adapter in load_as argument and activate it\n",
    "model.load_adapter(\"allenai/specter2_adhoc_query\", source=\"hf\", load_as=\"adhoc_query\", set_active=True)\n",
    "\n",
    "papers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\n",
    "          {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\n",
    "\n",
    "# concatenate title and abstract\n",
    "text_batch = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\n",
    "# preprocess the input\n",
    "inputs = self.tokenizer(text_batch, padding=True, truncation=True,\n",
    "                                   return_tensors=\"pt\", return_token_type_ids=False, max_length=512)\n",
    "output = model(**inputs)\n",
    "# take the first token in the batch as the embedding\n",
    "embeddings = output.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "435a8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame(\n",
    "    {\n",
    "    # \"DFG number\" : list(map(lambda x: x+1, list2[0])),\n",
    "    \"DFG question content\" : dfg_q,\n",
    "    }\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "for i,name in enumerate(model_name):\n",
    "    model = SentenceTransformer(name)\n",
    "    embedding_dfg = model.encode(dfg_q, convert_to_tensor=True)\n",
    "    embedding_he = model.encode(h_e_q, convert_to_tensor=True)\n",
    "    # compute similarity scores of two embeddings\n",
    "    cosine_scores = util.pytorch_cos_sim( embedding_dfg, embedding_he )\n",
    "    top_k=2\n",
    "    result_list = []\n",
    "    list3 = [[],[],[],[],[],[],[],[]]\n",
    "    q2 = h_e_q\n",
    "    q1 = dfg_q\n",
    "    for i, sentence in enumerate(q1):\n",
    "\n",
    "        top_results = np.argpartition(-cosine_scores.cpu()[i], range(top_k))[0:top_k]\n",
    "        result_list.append([i,int(top_results[0]) ])\n",
    "        list3[0].append(i)\n",
    "        list3[1].append(int(top_results[0]))\n",
    "        list3[2].append(float(max(list(cosine_scores[i]))))\n",
    "        list3[3].append(sentence)\n",
    "        list3[4].append(q2[top_results[0]])\n",
    "        list3[5].append(q2[top_results[1]])\n",
    "        list3[6].append(int(top_results[1]))\n",
    "        list3[7].append(float(cosine_scores[i][int(top_results[1])]))\n",
    "\n",
    "    #     result_list.append([i, int(top_results[1]) ])\n",
    "    #     print(\"DFG Question:\", sentence, \"\")\n",
    "    #     print(\"Top\", top_k, \"most similar sentences in DFG checklist:\")\n",
    "    #     for idx in top_results[0:top_k]:\n",
    "    #         print(q2[idx], \"Score:{:.4f}, No. of questions:{}\".format(cosine_scores[i][idx],idx+1 ))\n",
    "\n",
    "    #     print(\"\\n\")\n",
    "\n",
    "    df1[name+\"_1\" ] = list3[1]\n",
    "    df1[name+\"_2\" ] = list3[6]\n",
    "df1.to_csv(\"dfg_similar_questions_all_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bd88f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c049c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52454f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizon europe as \n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame(\n",
    "    {\n",
    "    # \"DFG number\" : list(map(lambda x: x+1, list2[0])),\n",
    "    \"Horizon Europe question content\" : h_e_q,\n",
    "    }\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "for i,name in enumerate(model_name):\n",
    "    model = SentenceTransformer(name)\n",
    "    embedding_dfg = model.encode(dfg_q, convert_to_tensor=True)\n",
    "    embedding_he = model.encode(h_e_q, convert_to_tensor=True)\n",
    "    # compute similarity scores of two embeddings\n",
    "    cosine_scores = util.pytorch_cos_sim( embedding_he, embedding_dfg )\n",
    "    top_k=2\n",
    "    result_list = []\n",
    "    list3 = [[],[],[],[],[],[],[],[]]\n",
    "    q1 = h_e_q\n",
    "    q2 = dfg_q\n",
    "    for i, sentence in enumerate(q1):\n",
    "\n",
    "        top_results = np.argpartition(-cosine_scores.cpu()[i], range(top_k))[0:top_k]\n",
    "        result_list.append([i,int(top_results[0]) ])\n",
    "        list3[0].append(i)\n",
    "        list3[1].append(int(top_results[0]))\n",
    "        list3[2].append(float(max(list(cosine_scores[i]))))\n",
    "        list3[3].append(sentence)\n",
    "        list3[4].append(q2[top_results[0]])\n",
    "        list3[5].append(q2[top_results[1]])\n",
    "        list3[6].append(int(top_results[1]))\n",
    "        list3[7].append(float(cosine_scores[i][int(top_results[1])]))\n",
    "\n",
    "    #     result_list.append([i, int(top_results[1]) ])\n",
    "    #     print(\"DFG Question:\", sentence, \"\")\n",
    "    #     print(\"Top\", top_k, \"most similar sentences in DFG checklist:\")\n",
    "    #     for idx in top_results[0:top_k]:\n",
    "    #         print(q2[idx], \"Score:{:.4f}, No. of questions:{}\".format(cosine_scores[i][idx],idx+1 ))\n",
    "\n",
    "    #     print(\"\\n\")\n",
    "\n",
    "    df1[name+\"_1\" ] = list3[1]\n",
    "    df1[name+\"_2\" ] = list3[6]\n",
    "df1.to_csv(\"he_similar_questions_all_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe52ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
